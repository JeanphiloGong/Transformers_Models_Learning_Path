\documentclass{book}
\usepackage{hyperref}
\usepackage{amsmath}



\begin{document}

\tableofcontents

\title{Linear Algebra}

% Chapter 1Introduction to Vectors
\chapter{Introduction to Vectors}

% 1.1.Vectors and Linear Combinations
\section{Vectors and Linear Combinations}
\subsection*{REVIEW OF THE KEY IDEAS}
    \begin{enumerate}
        \item A vector \( \mathbf{v} \) in two-dimensional space has two components \( v_1 \) and \( v_2 \).
        \item \( \mathbf{v} + \mathbf{w} = (v_1 + w_1, v_2 + w_2) \) and \( c\mathbf{v} = (cv_1, cv_2) \) are found a component at a time.
        \item A linear combination of three vectors \( \mathbf{u} \) and \( \mathbf{v} \) and \( \mathbf{w} \) is \( c \mathbf{u} + d\mathbf{v} + e\mathbf{w}\).
        \item Take all linear combination of \textbf{u}, or \textbf{u} and \textbf{v}, or \textbf{u, v, w}. In three dimensions, those combinations typically fill a line, then a plane, and the whole space \( \mathbf{R}^3\).
    \end{enumerate}

% 1.2.Lengths and Dot Products
\section{Lengths and Dot Products}
\subsection*{REVIEW OF THE KEY IDEAS}
    \begin{enumerate}
        \item The dot product \( \mathbf{v} \cdot \mathbf{w} \) multiplies each component \( v_i \) by \( w_i \) and adds all \( v_i w_i \).
        \item The length \( \| \mathbf{v} \| \) of a vector is the square root of \( \mathbf{v} \cdot \mathbf{v} \).
        \item \( \mathbf{u} = \frac{\mathbf{v}}{\| \mathbf{v} \|} \) is a \textbf{unit vector}, its length is 1.
        \item The cosine product is \( \mathbf{v} \cdot \mathbf{w} = 0 \) when vector \( \mathbf{v} \) and \( \mathbf{w} \) are perpendicular.
        \item The cosine of \( \theta \) (the angle between any nonzero \( \mathbf{v} \) and \( \mathbf{w} \)) never exceeds 1: \\
        \[
        \cos \theta = \frac{\mathbf{v} \cdot \mathbf{w}}{\| \mathbf{v} \| \| \mathbf{w} \|} \quad \text{(Schwarz inequality)} \quad | \mathbf{v} \cdot \mathbf{w} | \leq \| \mathbf{v} \| \| \mathbf{w} \|
        \]
         Problem 21 will produce the \textbf{triangle inequality} \( \| \mathbf{v} + \mathbf{w} \| \leq \| \mathbf{v} \| + \| \mathbf{w} \| \).
    \end{enumerate}



% 1.3.Matrices
\section{Matrices}

\subsection{Linear Equations}

\subsection{The Inverse Matrix}
this part is so hard

\subsection{Cyclic Differences}

\subsection*{REVIEW OF THE KEY IDEAS}
    \begin{enumerate}
        \item  \textbf{Matrix times vector:} \( \mathbf{Ax} = combination of the columns of A.\)
        \item The solution to \( Ax = b is x = A^{-1}b, when A is an invertible matrix.\)
        \item The difference matrix A is inverted by the sum matrix \( \mathbf{S = A^{-1}}\).
        \item The cyclic matrix \textbf{C} has no inverse. Its three columns lie in the same plane\\
        Those dependent columns add to the zero vector. \( \mathbf{Cx = 0}\) has many solutions.
        \item This section is looking ahead to key ideas. not fully explained yet.

    \end{enumerate}





% Chapter 2
\chapter{Solving Linear Equations}

% 2.1.Vectors and Linear Equations
\section{Vectors and Linear Equations}
\subsection*{REVIEW OF THE KEY IDEAS}
    \begin{enumerate}
        \item  
    \end{enumerate}

% 2.2.The Idea of Elimination
\section{The Idea of Elimination}

    % Course note
    \subsection*{Second lecture}
        \subsubsection{Elimination}
        \subsubsection{Back substitution}
        Here, we need to pay attention to the particularity of the matrix
        \begin{align}
            \begin{bmatrix}
            a & b \\
            c & d
            \end{bmatrix}
            \times
            \begin{bmatrix}
            e & f \\
            g & h
            \end{bmatrix}
            =
            \begin{bmatrix}
            ae + bg & af + bh \\
            ce + dg & cf + dh
            \end{bmatrix}
        \end{align}

    \subsection*{REVIEW OF THE KEY IDEAS}
        \begin{enumerate}
            \item  
        \end{enumerate}
   

%  2.3.Elimination Using Matrices
\section{Elimination Using Matrices}
    \subsection*{REVIEW OF THE KEY IDEAS}
        \begin{enumerate}
            \item  
        \end{enumerate}

% 2.4.Rules for Matrix operations
\section{Rules for Matrix Operations}
    \subsection*{REVIEW OF THE KEY IDEAS}
        \begin{enumerate}
            \item  
        \end{enumerate}

% 2.5.Inverse Matrices
\section{Inverse Matrices}
    \subsection*{REVIEW OF THE KEY IDEAS}
        \begin{enumerate}
            \item  
        \end{enumerate}
    
    % Course note
    \subsection*{Lecture 3}
    \subsubsection{Matrix multiplication}
    Four different way to slove matrices
        \begin{enumerate}
            \item \( C_{34} = (row_3 \,of\, A)(col_4\, of\, B) \). we can use a row of A times another column of B equal to a special product of a value of matrix c
            \item As before, we can use matrix A times each columns of B equal to matrix c
            \item We can use matrix A times each rows of matrix B equal to matrix c
            \item We can see \( AB = sum of (cols\, of\, A) \times (rows\, of\, B)\)
            \item Addition, we should know the textbf{Block multiplication} law
        \end{enumerate}
    \subsubsection{Inverse of A, AB, \( A^T\)}
    Inverses (square matrix) \\
    In this case, we should know if \( A^{-1}A = I = A A^{-1}\) exists, then A is invertible and nonsingular. If we were in a singular case, we can find \( A x = 0\) exist
    \subsubsection{\( \mathbf{Gauss-Jodan\, find\, A^{-1}}\)}
    Gauss-Jodan we can find \( E [AI] = [I A^{-1}]\)

% 2.6.Elimination = Factorization: A = LU
\section{Elimination = Factorization: A = LU}
    \subsection*{REVIEW OF THE KEY IDEAS}
        \begin{enumerate}
            \item  
        \end{enumerate}

% 2.7.Transposes and Permutations
\section{Transposes and Permutations}
    \subsection*{REVIEW OF THE KEY IDEAS}
        \begin{enumerate}
            \item  
        \end{enumerate}


% Chapter 3
\chapter{Vector Spaces and Subspaces}
\section{Spaces of Vectors}

\section{The Nullspace of A: Solving Ax = 0}

\section{The Rank and the Row Reduced Form}

\section{The Complete Solution to Ax = b}

\section{Independence, Basis and Dimension}

\section{Dimensions of the Four Subspaces}










% Chapter 4
\chapter{Orthogonality}
\section{Orthogonality of the Four Subspaces}

\section{Projections}

\section{Least Squares Approximations}

\section{Orthogonal Bases and Gram-Schmidt}







% Chapter 5
\chapter{Determinants}
\section{The Properties of Determinants}

\section{Permutations and Cofactors}

\section{Cramer's Rule, Inverse, and Volumes}








% Chapter 6
\chapter{Elgenvalues and Elgenvectors}
\section{Introduction to Eigenvalues}

\section{Diagonalizing a Matrix}

\section{Applications to Differential Equations}

\section{Symmetric Matrices}

\section{Positive Definite Matrices}

\section{Similar Matrices}

\section{Singular Value Decomposition}








% Chapter 7
\chapter{Linear Transformations}
\section{The idea of a Linear Transformation}

\section{The Matrix of a Linear Transformation}

\section{Diagonalization and the Pseudoinverse}









% Chapter 8
\chapter{Applications}
\section{Matrices in Engineering}

\section{Graphs and Networks}

\section{Markov Matrices, Population, and Economics}

\section{Linear Programming}

\section{Fourier Series: Linear Algebra for Functions}

\section{Linear Algebra for Statistics and Probability}

\section{Computer Graphics}










% Chapter 9
\chapter{Numerical Linear Algebra}
\section{Gaussian Elimination in Practice}

\section{Norms and Condition Numbers}

\section{Iterative Methods and Preconditioners}










% Chapter 10
\chapter{Complex Vectors and Matrices}
\section{Complex Numbers}

\section{Hermitian and Unitary Matrices}

\section{The Fast Fourier Transform}




\end{document}
